-- Reading data from external places:
    -- Note that some SQL systems such as data warehouses will have custom drivers. Spark will interact with various external databases differently, but the two basic approaches can be summarized as either:
    -- Moving the entire source table(s) to Databricks and then executing logic on the currently active cluster
    -- Pushing down the query to the external SQL database and only transferring the results back to Databricks
    -- In either case, working with very large datasets in external SQL databases can incur significant overhead because of either:
    -- Network transfer latency associated with moving all data over the public internet
    -- Execution of query logic in source systems not optimized for big data queries


--CTAS
/*
Create table: 
- Manual schema declaration, create empty table (need insert into)
- CTAS - do not support manual schema declaration (auto infer schema), table created with data
Useful for external data ingestion with well-defined schema. 

You cannot specify additional file options!!! Instead of CTAS you can use create table (schema) USING datasource. OPTIONS LOCATIOn
*/
CREATE TABLE new_table
  COMMENT "Contains PII"
  PARTITIONED BY (city, birth_date)
  LOCATON "/some/path"
  AS SELECT id, name, email, birth_date, city FROM users

/*
Table constraints
1) NOT NULL CONSTRAINT
2) CHECK CONSTRAINT
You can add during table creation
*/
CREATE TABLE people10m (
  id INT NOT NULL,
  firstName STRING,
  middleName STRING NOT NULL,
  lastName STRING,
  gender STRING,
  birthDate TIMESTAMP,
  ssn STRING,
  salary INT
);

ALTER TABLE people10m ALTER COLUMN middleName DROP NOT NULL;
ALTER TABLE people10m ALTER COLUMN ssn SET NOT NULL;

CREATE TABLE people10m (
  id INT,
  firstName STRING,
  middleName STRING,
  lastName STRING,
  gender STRING,
  birthDate TIMESTAMP,
  ssn STRING,
  salary INT
);

ALTER TABLE people10m ADD CONSTRAINT dateWithinRange CHECK (birthDate > '1900-01-01');
ALTER TABLE people10m DROP CONSTRAINT dateWithinRange;

ALTER TABLE people10m ADD CONSTRAINT validIds CHECK (id > 1 and id < 99999999);
DESCRIBE DETAIL people10m;
SHOW TBLPROPERTIES people10m;

-- PRIMARY KEY AND FOREIGN KEY CONSTRAINS ARE NOT ENFORCED
CREATE TABLE T(pk1 INTEGER NOT NULL, pk2 INTEGER NOT NULL,
                CONSTRAINT t_pk PRIMARY KEY(pk1, pk2));
CREATE TABLE S(pk INTEGER NOT NULL PRIMARY KEY,
                fk1 INTEGER, fk2 INTEGER,
                CONSTRAINT s_t_fk FOREIGN KEY(fk1, fk2) REFERENCES T);


-- EXTERNAL TABLE because we are specifying the LOCATION
-- This is a non-delta table. We are pointing to an external location! This is a table with external data source. We cannot expect the performance guarnatees associated with dela lake.

-- Solution, create a temp view then create a table using CTAS.
CREATE TABLE IF NOT EXISTS sales_csv
  (order_id LONG, email STRING, transactions_timestamp LONG, total_item_quantity INTEGER, purchase_revenue_in_usd DOUBLE, unique_items INTEGER, items STRING)
USING CSV
OPTIONS (
  header = "true",
  delimiter = "|"
)
LOCATION "${DA.paths.sales_csv}"

-- SOLUTION
CREATE TEMP VIEW temp_view_name (col1 STRING, col2 INT)
USING CSV
OPTIONS (
  header = "true",
  delimiter = "|"
)
LOCATION "${DA.paths.sales_csv}"

CREATE TABLE sales AS SELECT * FROM temp_view_name

CREATE TABLE events_json(key BINARY, offset LONG, partition INTEGER, timestamp LONG, topic STRING, value BINARY)
USING JSON 
LOCATION "${DA.paths.kafka_events}"


-- Read data from JDBC (this creates an external table) | CREATE DATASOURCE TABLE
    -- https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html
    -- https://docs.databricks.com/en/connect/external-systems/jdbc.html
    -- https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-create-table-datasource.html
    -- A Data Source table acts like a pointer to the underlying data source. For example, you can create a table “foo” in Spark which points to a table “bar” in MySQL using JDBC Data Source. When you read/write table “foo”, you actually read/write table “bar”.
    -- In general CREATE TABLE is creating a “pointer”, and you need to make sure it points to something existing. An exception is file source such as parquet, json. If you don’t specify the LOCATION, Spark will create a default table location for you.
    -- DROP TABLE IF EXISTS users_jdbc;
    -- The issue with tables created using is that their are external tables. They are not delta tables. Now, imaging you write something to 
    -- these tables using append. They query again. It wont read the appended data due to cached results. You need to run REFRESH table for this to pick up.

CREATE TABLE users_jdbc
USING JDBC
OPTIONS (
  url = "jdbc:sqlite:${DA.paths.ecommerce_db}",
  dbtable = "users"
)


-- Show schema
    -- Returns basic metadata information for qualified table `customer`
DESCRIBE TABLE sales_csv

    -- Returns additional metadata such as parent schema, owner, access time etc.
DESCRIBE TABLE EXTENDED sales_csv;

    -- Returns partition metadata such as partitioning column name, column type and comment.
DESCRIBE TABLE EXTENDED customer PARTITION (state = 'AR');

-- Query describe extended 
location = spark.sql("DESCRIBE EXTENDED users_jdbc").filter(F.col("col_name") == "Location").first()["data_type"]


-- TABLE PROPERTIES



